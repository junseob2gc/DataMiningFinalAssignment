*********************************************************
***************BDAT 1002 : Big Data Systems**************
*********************************************************
		   HDFS Commands
*********************************************************
### LOCAL FILE SYSTEM ###

see lesson slides for excercises
	ls
	mkdir
	cp
	mv
	rm



### LISTING ROOT DIRECTORY ###
-- First, list files under root in local directory

ls /

-- Here is the HDFS command to list the files in the root directory
-- Note that everything in HDFS starts with hadoop fs

hadoop fs -ls /

-- Notice that files are different in local filesystem and HDFS

### LISTING DEFAULT TO HOME DIRECTORY ###

-- The home directory in HDFS is under /user folder
-- In Linux, it is under /home

-- You can list the commands under the user directory using:

hadoop fs -ls

-- The above command is the same as:

hadoop fs -ls /user/cloudera

hadoop fs -copyFromLocal test.csv /user/cloudera/.

### CREATE A DIRECTORY IN HDFS ###

hadoop fs -mkdir /BDAT1002

-- Here we are creating something under the root directory (the / is called the root)
-- If you want to create something under the user directory you would execute this command:

hadoop fs -mkdir BDAT1002

hadoop fs -ls /
### COPY FROM LOCAL FS TO HDFS ###

-- create a test file called test.csv
-- Here is how we copy this file to out directory BDAT1002

hadoop fs -copyFromLocal  test.csv /BDAT1002

-- make sure the copy occurs by listing the files
hadoop fs -ls /BDAT1002

### COPY TO HDFS TO LOCAL FS ###

-- now let's do the opposite

hadoop fs -copyToLocal /BDAT1002/test.csv ./test10.csv

### CREATE 2 MORE DIRECTORIES ###

hadoop fs -mkdir /BDAT1002/hadoop-test2

hadoop fs -mkdir /BDAT1002/hadoop-test3

### COPY A FILE FROM ONE FOLDER TO ANOTHER ###

-- copy a file from BDAT1002 diretctory to hadoop-test2 directory

hadoop fs -cp /BDAT1002/test.csv /BDAT1002/hadoop-test2

-- FYI, the above is equivalent to

hadoop fs -cp /BDAT1002/test.csv /BDAT1002/hadoop-test2/

hadoop fs -cp /BDAT1002/test.csv /BDAT1002/hadoop-test2/.



### MOVE A FILE FROM ONE FOLDER TO ANOTHER ###

-- this is a move not a copy

hadoop fs -mv /BDAT1002/test.csv /BDAT1002/hadoop-test3

### CHECK REPLICATION ###

-- How do we find the replication in a directory?
-- if you do a listing on a folder, it will show the replication factor 

hadoop fs -ls /BDAT1002

### CHANGE OR SET REPLICATION FACTOR ###

-- replication factor is 3 by default, but you can change it using the -Ddfs.replication property as shown below
-- Here I am making a copy of the file test.csv but changing the replication factor from default 3 to 2

hadoop fs -Ddfs.replication=2 -cp /BDAT1002/test.csv /BDAT1002/hadoop-test2/test_with_rep2.csv

### CHANGING PERMISSIONS ###

-- same as linux, use chmod

hadoop fs -chmod 777 /BDAT1002/test.csv

### FILE SYSTEM CHECK - REQUIRES ADMIN PREVILEGES ###

-- You can get more information about a file by using the fsck - file system check command
-- you can do this at a folder level 

sudo -u hdfs hdfs fsck hadoop-test2 -files -blocks -locations 


-- or at the file level

sudo -u hdfs hdfs fsck /BDAT1002/test.csv -files -blocks -locations 

### Location of blocks in local file system ###

gedit /etc/hadoop/conf/hdfs-site.xml

-- The locations of the blocks is listed under dfs.datanode.data.dir

-- location of blocks

/var/lib/hadoop-hdfs/cache/hdfs/dfs/data/current/BP-1067413441-127.0.0.1-1508775264580/current/finalized/subdir0/subdir0

-- Remember, your local file directory can only see the blocks, whereas HDFS knows the location of the files