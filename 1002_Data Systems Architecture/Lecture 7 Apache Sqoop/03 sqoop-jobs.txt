*********************************************************
***************BDAT 1002 : Big Data Systems**************
*********************************************************
		Sqoop - Incremental Imports
*********************************************************


## Classic Import with Selection ###

sqoop import --connect jdbc:mysql://localhost/stocks_db --username root --password cloudera --table stocks --columns "symbol,name,trade_date,volume" --where "id > 10" -m 1 --target-dir /BDAT1002/sqoop/stocks_selective

## Incremental Import - Append ##

sqoop job --create incrementalImportJob -- import --connect jdbc:mysql://localhost/stocks_db --username root --password cloudera --table stocks --target-dir /BDAT1002/sqoop/stocks_append --incremental append --check-column id

sqoop job --list

sqoop job --show incrementalImportJob

sqoop job --exec incrementalImportJob

hadoop fs -ls /BDAT1002/sqoop/stocks_append
hadoop fs -cat /BDAT1002/sqoop/stocks_append/part-m-00000

INSERT INTO stocks 
(symbol, name, trade_date, close_price, volume)
VALUES
('AAL', 'American Airlines', '2015-11-12', 42.4, 4404500);

INSERT INTO stocks 
(symbol, name, trade_date, close_price, volume)
VALUES
('AAPL', 'Apple', '2015-11-12', 115.23, 40217300);

INSERT INTO stocks 
(symbol, name, trade_date, close_price, volume)
VALUES
('AMGN', 'Amgen', '2015-11-12', 157.0, 1964900);

-- Make sure rows were added

SELECT * FROM stocks;


-- Run sqoop job again

sqoop job --exec incrementalImportJob

-- Look up the details of the incremental job

sqoop job --show incrementalImportJob


hadoop fs -ls /BDAT1002/sqoop/stocks_append
hadoop fs -cat /BDAT1002/sqoop/stocks_append/part-m-00004


## Incremental Import - lastmodified ##

-- create sqoop incremental job

sqoop job --create incrementalImportModifiedJob -- import --connect jdbc:mysql://localhost/stocks_db --username root --password cloudera --table stocks --target-dir /BDAT1002/sqoop/stocks_modified --incremental lastmodified --check-column updated_time -m 1 --append

-- list jobs

sqoop job --list

sqoop job --show incrementalImportModifiedJob

sqoop job --exec incrementalImportModifiedJob

hadoop fs -ls /BDAT1002/sqoop/stocks_modified
hadoop fs -cat /BDAT1002/sqoop/stocks_modified/part-m-00000

-- Now update the stocks table

UPDATE stocks SET volume = volume+100, updated_time=now() WHERE id in (10,11,12);

INSERT INTO stocks 
(symbol, name, trade_date, close_price, volume)
VALUES
('GARS', 'Garrison', '2015-11-12', 12.4, 23500);

INSERT INTO stocks 
(symbol, name, trade_date, close_price, volume)
VALUES
('SBUX', 'Starbucks', '2015-11-12', 62.90, 4545300);

INSERT INTO stocks 
(symbol, name, trade_date, close_price, volume)
VALUES
('SGI', 'Silicon Graphics', '2015-11-12', 4.12, 123200);

-- make sure update occured

SELECT * FROM stocks;

sqoop job --show incrementalImportModifiedJob

sqoop job --exec incrementalImportModifiedJob

hadoop fs -ls /BDAT1002/sqoop/stocks_modified

-- should only have 6 records

hadoop fs -cat /BDAT1002/sqoop/stocks_modified/part-m-00001

### MERGE ###

-first we need to create a jar file with a java class file which represents the stocks table
-- it is easy to create using this sqoop codegen command
--  This is how it works: you say sqoop codegen, give mysql location, give the table name for which you need to create the jar file with teh java class
-- and in the out directory, give the location in the LOCAL file director

sqoop codegen --connect jdbc:mysql://localhost/stocks_db --username root --password cloudera --table stocks --outdir /home/cloudera/sqoop-codegen-stocks

-- copy the created jar file to the home directory
cp /tmp/sqoop-cloudera/compile/f55d715ab943d0d7accf1c4551260605/stocks.jar .

-- we will now use this jar file in our sqoop merge command
-- for sqoop merge, we need to provide two files as import
-- in the new data data option, you need to give the most recent import.
-- in the onto option you need to specify the file which you will like to merge the content 
-- We also need to give the jar file and the class name
-- The merge key has to be a unique column in the table (primary key)

sqoop merge --new-data /BDAT1002/sqoop/stocks_modified/part-m-00001 --onto /BDAT1002/sqoop/stocks_modified/part-m-00000 --target-dir /BDAT1002/sqoop/stocks_modified/merged --jar-file stocks.jar --class-name stocks --merge-key id

-- Let's look at the output directory

hadoop fs -ls /BDAT1002/sqoop/stocks_modified/merged

-- we see only one file

hadoop fs -cat /BDAT1002/sqoop/stocks_modified/merged/part-r-00000